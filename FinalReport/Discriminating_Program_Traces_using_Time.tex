\documentclass{article}

\usepackage{amsmath,amsfonts,amsthm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{xcolor,pgf,tikz,pgflibraryarrows,pgffor,pgflibrarysnakes}

\usetikzlibrary{fit} % fitting shapes to coordinates
\usetikzlibrary{backgrounds} % drawing the background after the foreground

\tikzstyle{background}=[rectangle,fill=gray!10, inner sep=0.1cm, rounded corners=0mm]
    
\usepgflibrary{shapes}
\usetikzlibrary{snakes,automata}


\usepackage{tikz}
\tikzstyle{nloc}=[draw, text badly centered, rectangle, rounded corners, minimum size=2em,inner sep=0.5em]
\tikzstyle{loc}=[draw,rectangle,minimum size=1.4em,inner sep=0em]
\tikzstyle{trans}=[-latex, rounded corners]
\tikzstyle{trans2}=[-latex, dashed, rounded corners]

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\usetikzlibrary{automata,positioning}
\newcommand{\msot}{\textnormal{MSOT}} 
\newcommand{\twst}{\textnormal{2WST}}
\newcommand{\la}{\ensuremath{{\mathrm{la}}}}
\newcommand{\nsst}{\wnsst{}}
\newcommand{\wsst}{\textnormal{SST}} 
\newcommand{\wnsst}{\textnormal{NSST}} 
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{subcaption}
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{shapes.arrows}
\usepackage{array}
\usepackage{stmaryrd}
\DeclareCaptionFormat{myformat}{#3}
\captionsetup[algorithm]{format=myformat}

\begin{document}
\title{Discriminating Program Traces using Time}
\author{Saeid Tizpaz Niari, Shyam Sundar Ramamoorthy, Yue Zhang \\ Final Report of Machine Learning (CSCI 5622)  \\ Fall 2016}
\date{}
\maketitle

\section{Introduction}
Different control-flow paths in a program can have varying execution times.
Such observable differences in execution times may be explainable by
information about the program internals, such as whether or not a given function or
functions were called. 
How can a software developer (or security analyst)
determine what internals may or may not explain the varying
execution times of the program? In this project, we consider the problem
of helping such a developer 
or analyst to identify such explanations. 

Given a set of execution traces with observable execution times grouped
into a finite set of labels, a \emph{discriminant} (or a classifier model) is
a map relating each label to a set of features. Such a discriminant model can then be used,
for example, to explain timing differences among program traces based on
its internal features like method calls.  In other words, this problem looks for
distinguishing features among traces that result in
varying execution times.

Crucially, once we can explain the timing
differences in terms of properties of traces (i.e., control-flow paths through
the program), the analyst can use the explanation to diagnose the possible
timing side-channel and potentially find a fix for the vulnerability. 

Our overall approach is to first obtain a set of execution traces
with information recorded about internal features of the input program
along with corresponding execution times. We consider two main
approaches to explain time differences based on the internal features: \\
1- We cluster these training traces based on their overall execution times
to bin them into timing labels. Then, we learn a decision tree discriminant
model from these traces to capture what is common amongst the traces with the
same timing labels and what is different between traces with different labels. \\
2- As the time of execution is a continuous domain (time $\in$ $R$), we can
naturally use regression trees to find discriminant features of the input
program and explain the timing differences. \\
Moreover, we compare these
two approaches and explain which one can potentially give better discriminant. 
Finally, we apply other classifier models and measure their accuracy and performance.

We present two case studies in identifying and debugging timing
side-channel shown our tools to
perform label clustering and decision tree-discriminant learning. We show that
the decision trees are useful for explaining the timing
differences amongst trace sets and performing this debugging task.

Some technicalities of our approach include that we need to execute both an
instrumented and uninstrumented version of the program of interest: a trace of
the instrumented program is needed to determine which functions are called during
the trace execution, while the execution time of interest is for an uninstrumented
program. Since timing observations are noisy due to many sources of
non-determinism, we also provide this option that each trace is associated with a
\emph{distribution} over the labels.
For instance, a trace may have a label $\ell_1$ with probability $0.9$
and label $\ell_2$ with probability $0.1$.

\section{Motivation Example - existing user enumeration}


\section{Motivation Example - SnapBuddy (social network)}
In this section, we demonstrate another example to show that the tool can be useful in
identifying timing side-channel vulnerabilities
and suggesting ways to fix them.
We use an application called
SnapBuddy as an example. SnapBuddy is a Java application with 3,071 methods,
implementing a mock social network in which
each user has their own page with a photograph. Figure \ref{snapbuddy-1} shows
a screenshot of SnapBuddy application. Each user logs into system observes the
list of his/her friends, can add new friends, and overview public profile pages of
other users. 

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{snapbuddy-1}
    \caption{A screenshot of SnapBuddy web application}
    \label{snapbuddy-1}
\end{figure}

\subsubsection*{Identifying a Timing Side-Channel with Clustering.}
As we mention, every user can issue download requests to the
public profile page of other uses in the system. The public profile
includes the users' public information and their public profile page. 
As an analyst, we record the time to download public pages of all users
in the system. Figure~\ref{fig:sbtime} shows a scatter plot of the running times of
various traces with each trace represented by a point in the figure. The
running times are clustered into $6$ different groups using a
standard $k$-means clustering algorithm and shown using different
colors. We see that for some users, the download times were roughly $15$ seconds, whereas for
some others they were roughly $7.5$ seconds. This significant time differential suggests a
potential timing side-channel if the difference can be correlated with sensitive
program state and thus this differential should be investigated further with the tool.

To see how such a time differential could be a timing side-channel,
let us consider an attacker that
(a) downloads the public profile pages of all users and learns each download time,
and (b) can observe timing between packets by sniffing the
network traffic between legitimate users.
If the attacker observes user Alice downloading the page of another user whose identity
is supposed to be a secret
and sees that the download took approximately $7.5$ seconds, the attacker can infer
that Alice downloaded the page of one of the six users corresponding
to the six red squares in Figure~\ref{fig:sbtime}. The timing
information leak thus helped the attacker narrow down the
possibilities from hundreds of users to six.

\subsubsection*{Explaining Timing Side-Channels with Decision Tree Learning.}
Recall that the analyst downloaded pages
of all the users. Now the same download queries are executed over an
instrumented version of the SnapBuddy server to record the number of
times each method in the application is called by the trace. As a
result, we obtain a
set of traces with their (uninstrumented) overall running times and their
sequence of method calls.

\begin{figure}[t]
\centering
\begin{minipage}[b]{0.47\textwidth}
  \centering
  \includegraphics[width=\textwidth]{SnapBuddy_scatter_2}
  \caption{Cluster running times from the SnapBuddy to produce labels. The scatter plot shows a differential corresponding to a possible timing side-channel.}
  \label{fig:sbtime}
\end{minipage}\hfill
\begin{minipage}[b]{0.50\textwidth}
  \centering
  \includegraphics[width=\textwidth]{SnapBuddy_DecisionTree_1}
  \caption{Snippet of a decision-tree discriminant learned from SnapBuddy traces using the timing labels from Figure~\ref{fig:sbtime}.}
  \label{fig:sbdectree}
\end{minipage}
\vspace{-2em}
\end{figure}

Then tool uses the
standard \emph{CART} decision tree learning
algorithm to infer a decision tree that
succinctly represents a discriminant
using atomic predicates that characterize
whether or not the trace invoked a particular method (shown in Figure~\ref{fig:sbdectree}).
For instance, the cluster representing the longest running time (around $15$ seconds) is
discriminated by calling functions of \texttt{snapservice.model.Filter.filter} and 
\texttt{image.OilFilter.filterPixels}, indicating that the two methods
are both invoked by the trace. Likewise, the cluster representing the
running time around $7.5$ seconds is discriminated by the
property \texttt{snapservice.model.Filter.filter} and
not \texttt{image.OilFilter.filterPixels} and
\texttt{image.ChromeFilter.filter}, indicating that
\texttt{image.OilFilter.filterPixels} must not invoked
while the other two are.

The analyst might now suspect what is going on: the timing
differences are caused by the filters that each user chooses to apply
to their picture. %To illustrate the filters, Figure~\ref{figure2-1}
%shows a user's image transformed by the Chrome filter (middle) and Oil
%filter (right).
Note that the analyst running the tool did not need to
know that the filters are important for causing this time differential, or even that
they existed. The tool discovers them simply because the trace
contains all method calls, and the decision tree learning algorithm
produces a useful discriminant.

A possible fix now suggests itself: make sure that the execution of
each type of filter takes the same amount of time.

\section{Methodology}
In this section, we explain the steps and process to produce a discriminant
decision tree model using the example of SnapBuddy described in the previous
section. Figure \ref{Methodology} shows the steps of processing on the SnapBuddy
example. As we described in the previous sections, the records (traces ) of data sets from
input applications include time measurements (uninstrumented version) and function
calls (instrumented version using Soot Framework) under the same user input. As noted before,
we are interested in seeing whether the key
methods that explain the differences in execution time can be pinpointed. Thus, we
consider attributes corresponding to the called methods in a trace. This data set for SnapBuddy
is the input for the process model of Figure \ref{Methodology} (note that the Figure shows
only part of function calls). 
Let's look at the process step by step: \\
1. \smallskip\textit{Clustering and Class Label: Total Execution Time Ranges.}
To identify the most salient attributes, we fix a small number of possible
labels, and cluster traces according to total execution time. 
Each cluster is defined by a corresponding time interval. The clusters and
their intervals are learned using $k$-means clustering
algorithm. In this case, we fix $k$=6 and cluster the data set as shown in the
result of $step.1$. We consider the execution time for each trace to be a
random variable and assume a normal distribution.
We obtain the mean and variance through $10$ repeated measurements.
We apply clustering to the mean execution times of each trace to
determine the class labels. A class label (or cluster) can be identified by the mean of
all execution times belonging to that cluster. Then, considering the
class labels sorted in increasing order, we define the lower boundary
of a bucket for classifying new traces by averaging the maximum
execution time in the previous bucket and the minimum execution time
in this bucket (and analogously for the upper boundary). 
\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{Methodology}
    \caption{An overall view of the tool's process for clustering and classification of input data set}
    \label{Methodology}
\end{figure}
\\2. \smallskip\textit{Estimation of trace weights.}
Given a set of time ranges, we define a weighted labeling of traces
that permits a trace to be assigned to different clusters with different weights.
For a given trace, the weights to clusters are determined by the
probability mass belong to the time range of the cluster.  For
example, consider the execution time distribution of a sample trace t1
(drawn from our SnapBuddy example discussed below) shown in
this step. This distribution straddles the
cluster$\_$0 and cluster$\_$1 boundary.  In this case, we assign trace
t1 to both cluster$\_$0 (black area) and cluster$\_$1 (blue area) with
weights according to their probability mass in their respective
regions. In this example, trace t1 is assigned
to cluster$\_$0 with weight 22 and cluster$\_$1 with weight 78. Note
that this provides a smoother interpretation of the class labels
rather than, for example, assigning the most likely label for each
trace. Please note that the tool can also handle the case where
there is just one time measurement, and each trace belongs to a
fix cluster. \\
3. \smallskip\textit{data set of trace labels and weights.}
Given that $step.1$ will assign labels on traces and $step.2$
calculate the weight of the trace to belong to different labels,
we can obtain the new data set which is the input for classification step. 
We keep the function call features like the original input data set, but we
find labels and weights for each trace as described in $step.2$ and $step.3$
and generate the data set as shown in this step for SnapBuddy. \\
4. \smallskip\textit{Decision Tree Learning.}
From a training set with this weighted labeling, we apply the weighted
decision tree learning algorithm CART.
We use the tool chain both for clustering in the time domain as described
above to determine the class labels and weights of each trace and for
learning the classification model. In addition, we use group k-fold cross
validation procedure to find accuracy. 
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{Regression_Tree}
    \caption{Regression Tree: Results of applying regression tree algorithm on SnapBuddy}
    \label{Regression_Tree}
\end{figure}

The another approach is to apply regression tree directly. Especially,
we know that time is continuous domain. Therefore, we can naturally
apply regression tree on our data sets. Figure \ref{Regression_Tree}
shows the results of applying regression tree on SnapBuddy data set.
As you may notice, on each leaf node, we have the predicted value for
time. For example, if $image.OilFilter.filterPixels$ is called (the right child taken), 
then the predicted time value is about 14.7 seconds which corresponds to
traces which called $oilFilter$ function. In the case of decision tree classifier,
the leaf node classifies the path to one of possible labels. 



\section{Estimation of Cluster Numbers}

In order to apply K nearest neighbors classifiers we need to have a rough estimate of how many clusters can be form from the given dataset. Therefore, we choose DBSCAN to estimate the number of clusters. Density-based spatial clustering of applications with noise groups data samples by identifying core points and their reachable neighbors.

The idea behind the algorithm of DBSCAN can be elaborated as the following:

\begin{enumerate}
    \item
    Determining two parameters $\epsilon$ and the minimum number of samples required to form a cluster. In our implementation, we define $\epsilon=0.3$ and the number of minimum samples is 10.

    \item
    The algorithm scan each data sample that hasn't been marked unvisited, find $\epsilon-neighbors$ of this point, if the number of points reach the minimum baseline, a new cluster is formed. Otherwise, this point is marked as noise with label $-1$ in our implementation.

    \item
    If a point in $\epsilon-neighbors$ is also the core point of another cluster, we union these two clusters to form a new one.

    \item
    The above processes repeats until all data samples are visited and therefore, we get the dataset being clustered with different labels.
\end{enumerate}

    \includegraphics[scale=0.2]{MSB1_10_100.png}
    \includegraphics[scale=0.2]{RegEx_20_10_1_101.png}

    \includegraphics[scale=0.2]{RegEx_150_10_2_10011.png}
    \includegraphics[scale=0.2]{RegEx_200_10_2_101011.png}

Results and explanations:

\begin{tabular}{ |p{2cm}||p{1.8cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}| }
 \hline
 \multicolumn{5}{|c|}{DBSCAN Results} \\
 \hline
 Measurements & MSB1\_10 & RegEx\_20 &RegEx\_150 & RegEx\_200\\
 \hline
 Estimated number of clusters   & 9 & 6 & 51 & 69\\
 Homogeneity & 0.499 & 0.333 &0.496 & 0.489\\
 Completeness & 1.000 & 1.000 & 1.000 & 1.000\\
 V-measure & 0.666 & 0.500 & 0.663 & 0.656\\
 Adjusted Rand Index & 0.187 & 0.003 & 0.001 & 0.000\\
 Silhouette Coefficient & 1.000 & 0.616 & 0.791 & 0.718\\
 \hline
\end{tabular}\\

\begin{enumerate}

    \item dataset

    The applications that we collect data from are two micro-benchmark applications which are vulunerable to side-channels. As It takes different time for different function calls, the programs are potential to leak secrets. In this case, the inputs are a set of bit sequence of 0 and 1. Given the inputs, the programs will call functions which are determined by pattern of the benchmark:

    \begin{enumerate}
        \item
        MSB: Only one function is called at each time. Therefore input data is sequence of 0 or 1 bits with 10 different function calls as features.

        \item
        Reg: For regular expression datset we are looking for patterns of several function calls each time. RegEx\_20, RegEx\_150 and RegEx\_200 in the above table are datasets with 20, 150 and 200 different function calls, looking for patterns 101, 10011 and 101011 respectively.

    \end{enumerate}
    \item Parameters

    Homogeneity score explains the purity of data in clusters. Therefore, the higher homogeneity score is the better performance of clustering algorithm has. For the first three datasets, because the combination of functions is more complicated than MSB data, the homogeneity is not very high when features are simple. However, homogeneity grows as features increases.

    Completeness score explains whether a cluster contains all the data points of a given class.

    V-measure is the harmonic mean between homogeneity and completeness. A perfect labeling is both homogeneity and completeness and thus is 1.0.

    Rand Index is very low here since the input matrix is sparse and didnâ€™t consider the combination of functions as a feature. Therefore, it thinks samples in the same class are classified into different classes.

    Silhouette Coefficient is the best when equals to 1 and the worst when equals to -1. Closed to 0 indicates there are overlapping clusters.

\end{enumerate}




\section{Other Regression and Classifiers}

\begin{enumerate}
    \item Regression

    For linear regression, we split 80\% of each data into training set. By fitting the data in a regression model we get the following scores. The accuracy of MSB data is almost 1.0 since linear pattern is very obvious with only one function call at each time. The accuracy on regular expression dataset decrease obviously when model becomes complex.

\begin{tabular}{ |p{2cm}||p{1.8cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}| }
 \hline
 \multicolumn{5}{|c|}{Linear Regression Model} \\
 \hline
 Scores & MSB1\_10 & RegEx\_20 &RegEx\_150 & RegEx\_200\\
 \hline
 Variance Score   & 0.99 & 0.97 & -1.63e+19 & -2.22e+19\\
 MSE & 2.28 & 4870613.93 & $2.26*10^{29}$ & $1.16*10^{30}$\\
 \hline
\end{tabular}\\

    \item Classification

    For classification problems we applied nearest neighbors, linear SVM, RBF SVM, decision tree, random forest, neural network, adaboosting and Naive bayes classifiers. The table below indicates that naive bayes, nearest neighbors and RBF SVM have the best performance.

\begin{tabular}{ |p{2cm}||p{1.8cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}| }
 \hline
 \multicolumn{5}{|c|}{Linear Regression Model} \\
 \hline
 Scores & MSB1\_10 & RegEx\_20 &RegEx\_150 & RegEx\_200\\
 \hline
 Nearest Neighbors   & 1.0 & 0.1 & 0.08 & 0.09\\
 Linear SVM & 0.56 & 0.1 & 0.03 & 0.06\\
 RBF SVM & 1.0 & 0.2 & 0.1 & 0.11\\
 Decision Tree & 0.81 & 0.15 & 0.04 & 0.06\\
 Random Forest & 0.98 & 0.17 & 0.08 & 0.06\\
 Neural Net & 0.98 & 0.2 & 0.04 & 0.07\\
 Adaboost & 0.98 & 0.2 & 0.03 & 0.06\\
 Naive Bayes & 1.0 & 0.23 & 0.09 & 0.09\\
 \hline
\end{tabular}\\


\end{enumerate}
\section{Results}
As described earlier, we are looking for a scalable and accurate model which should
be able to explain the data given program internal function calls as features and time
of trace execution as target function. We observe that decision tree classifier and
regression tree give a reasonable explanation for the input data and have good
accuracy and high scalability. We also consider the compactness of model in
terms of height of tee. Table \ref{table7-1} shows the results of applying decision
tree classifier on the applications and micro-benchmarks we have used in this project.
It is worth nothing to mention that explaining the data with good accuracy is more
important to us in comparison to less explanatory models with high accuracy. But, we
get high accuracy using decision tree classifier. 

\begin{table}[t]
  \centering
  \caption{Application and Micro-benchmark results for decision-tree discriminators learned
    using decision tree. 
    Legend: \textbf{\#M}: number of methods, \textbf{\#N:} number of traces,
    \textbf{T}:\ computation time in seconds, \textbf{A}: accuracy, 
    \textbf{H}: decision-tree height % denotes a time less than
    %$0.1$ seconds.
   }
  \label{table7-1}
  \begin{tabular}{ || l | r | r || r | r | r ||}
    \hline
    &       &    & \multicolumn{3}{c||}{Decision Tree}\\
    ~~~~Application Name~~~~~~ & \# \textbf{M} & \#\textbf{N} & \textbf{T} & \textbf{A} & \textbf{H} \\ \hline
     $Existing User Enumeration$ & 2 & 1000 & 0.01 & 100\% & 2  \\ \hline
    $SnapBuddy$ & 160 & 439 & 0.01 & 100\% & 8  \\ \hline
    $MSB0$ & 10 & 188 & 0.01 & 100\% & 7  \\ \hline
    $RegE_{101}$ & 20 & 200 & 0.01 & 100\% & 13 \\ \hline
    $RegE_{10011}$ & 150 & 1500 & 0.5 & 89.2\% & 44  \\ \hline
    $RegE_{101011}$ & 200 & 2000 & 0.8 & 92.1\% & 50 \\ \hline
    \end{tabular}
\end{table}

\section{Conclusion}
In this project, we show that combination of program analysis and machine learning can help
security analysts to find explanation for possible execution time difference in program
control flow paths. Such explanation is useful to eliminate vulnerabilities to timing side-channels.
We show our approaches with two real applications and micro-benchmarks. In particular, we show
that clustering data in time domain (obtaining class labels) and classification of them using
decision tree discriminant is a useful approach to explain time difference based on program
internal features. We also show that regression tree ca be also useful to explain the data. In
addition, we explain the results we get from other machine learning algorithm but they are
less explainable. The challenges include finding the number of clusters and correlation
of features in the data set. We use estimation of cluster numbers to find more accurate
value for k in k-means algorithm, and we consider expert knowledge to select useful
features among correlated ones. Possible extensions include considering the case such
that order of function calls is important. So, we may need to learn graph instead of tree
in this case. The another extension is to consider two approaches of classification with
clustering or direction regression models. Even though time belong to continuous domain,
and it naturally makes sense to apply regression tree, consider our problem, it seems that
in some cases, classification with clustering capture relationships between features and
time better. Especially, as regression tree splits upon a node in each step, we may loose
some relationships between program internal features to each other by applying regression
directly. However, we need to investigate more in this area to have concrete results.  


\end{document}
